================================================
Phishing Detection - Machine Learning Pipeline Execution
================================================

Python version: Python 3.13.9

Found virtual environment: venv
Virtual environment activated.
Python: /Users/ymca/_dev_work_/venv/bin/python

Checking critical dependencies...
‚úÖ Critical dependencies are installed.

Using configuration file: config.yaml

Executing Machine Learning Pipeline...

======================================================================
MACHINE LEARNING PIPELINE INITIALIZED
======================================================================
Config loaded: 7 sections
Database URL: https://YOUR_STORAGE_ENDPOINT/phishing.db
Test Size: 0.2
Random State: 42
Enabled Models: 4 models
======================================================================

======================================================================
PIPELINE EXECUTION STARTED
======================================================================

[Step 1/7] Loading data from SQLite database...
======================================================================
STEP 2: Loading Dataset
======================================================================
‚úì Using existing database file
üìä Table name: phishing_data
‚úì Removed index column (Unnamed: 0)

üìà Dataset Summary:
   ‚Ä¢ Shape: 10,500 samples √ó 15 features
   ‚Ä¢ Features: 14
======================================================================
======================================================================
STEP 3: Feature and Target Separation
======================================================================

üìä Features Summary:
   ‚Ä¢ Shape: 10,500 samples √ó 14 features
   ‚Ä¢ Feature columns: 14
======================================================================
======================================================================
HANDLING MISSING VALUES
======================================================================

üìä Columns with missing values: 1
   ‚Ä¢ LineOfCode: 2,355 (22.43%)

‚úì Created LineOfCode_Missing indicator variable
‚úì Imputed LineOfCode with median: 620.00

‚úì Missing value handling complete
======================================================================
======================================================================
FEATURE TYPE IDENTIFICATION
======================================================================

üìä Categorical Features (2):
   ‚Ä¢ Industry
   ‚Ä¢ HostingProvider

üìä Numerical Features (13):
   ‚Ä¢ LineOfCode
   ‚Ä¢ LargestLineLength
   ‚Ä¢ NoOfURLRedirect
   ‚Ä¢ NoOfSelfRedirect
   ‚Ä¢ NoOfPopup
   ‚Ä¢ NoOfiFrame
   ‚Ä¢ NoOfImage
   ‚Ä¢ NoOfSelfRef
   ‚Ä¢ NoOfExternalRef
   ‚Ä¢ Robots
   ‚Ä¢ IsResponsive
   ‚Ä¢ DomainAgeMonths
   ‚Ä¢ LineOfCode_Missing

üìà Categorical Feature Cardinality:

   Industry:
      ‚Ä¢ Unique values: 11
      ‚Ä¢ Top 5 categories: ['eCommerce', 'Non-profit', 'Education', 'Unknown', 'Fashion']

   HostingProvider:
      ‚Ä¢ Unique values: 13
      ‚Ä¢ Top 5 categories: ['Unknown Provider', 'GoDaddy', 'Bluehost', 'HostGator', 'Freehostia']
======================================================================
   ‚úì Data loaded: 10,500 samples, 15 features
   ‚úì Numerical features: 13
   ‚úì Categorical features: 2

[Step 2/7] Preprocessing data and splitting into train/test sets...
======================================================================
STEP 4: Train-Test Split
======================================================================

üìä Dataset Split:
   ‚Ä¢ Training set: 8,400 samples (80.0%)
   ‚Ä¢ Test set: 2,100 samples (20.0%)

üìà Training Set Class Distribution:
   ‚Ä¢ Legitimate (1): 4,623 (55.04%)
   ‚Ä¢ Phishing (0): 3,777 (44.96%)

üìà Test Set Class Distribution:
   ‚Ä¢ Legitimate (1): 1,156 (55.05%)
   ‚Ä¢ Phishing (0): 944 (44.95%)

‚úì Data split complete with stratification
======================================================================
======================================================================
PREPROCESSING PIPELINE CREATED
======================================================================
‚úì Pipeline Components:
   ‚Ä¢ Numerical features (13): RobustScaler (robust to outliers)
   ‚Ä¢ Categorical features (2): OneHotEncoder (drop='first')
======================================================================

‚úì Preprocessing applied to training data
   Original shape: (8400, 15)
   Transformed shape: (8400, 35)
‚úì Preprocessing applied to data
   Original shape: (2100, 15)
   Transformed shape: (2100, 35)
   ‚úì Preprocessing complete
   ‚úì Training shape: (8400, 35)
   ‚úì Test shape: (2100, 35)

[Step 4/7] Training models...
======================================================================
STEP 5: MODEL DEFINITIONS
======================================================================

üìä Creating 4 models...
   ‚úì Logistic Regression: LogisticRegression
   ‚úì Random Forest: RandomForestClassifier
   ‚úì Gradient Boosting: GradientBoostingClassifier
   ‚úì XGBoost: XGBClassifier
======================================================================
======================================================================
TRAINING MACHINE LEARNING MODELS
======================================================================

üìö Training Process Explanation:
----------------------------------------------------------------------

Different models use different training approaches:

1. **Logistic Regression**:
   ‚Ä¢ Uses iterations (max_iter=1000) until convergence
   ‚Ä¢ Automatically stops when loss converges
   ‚Ä¢ Linear classifier, interpretable, fast baseline

2. **Random Forest**:
   ‚Ä¢ Builds multiple trees independently (n_estimators=100)
   ‚Ä¢ No iterative refinement needed
   ‚Ä¢ Parallel tree construction, robust to outliers

3. **Gradient Boosting**:
   ‚Ä¢ Uses boosting rounds (n_estimators=100)
   ‚Ä¢ Sequentially builds trees/estimators
   ‚Ä¢ Each tree corrects errors of previous trees

4. **XGBoost**:
   ‚Ä¢ Optimized gradient boosting with regularization
   ‚Ä¢ Uses boosting rounds (n_estimators=100)
   ‚Ä¢ Handles missing values natively, fast and scalable


‚öôÔ∏è  Hyperparameter tuning: ENABLED
   Method: RandomizedSearchCV
   Iterations: 50
   CV folds: 5

[1/4] Training Logistic Regression...
   üîç Performing hyperparameter tuning (n_iter=50, cv=5)...
   ‚úì Best CV score: 0.8462
   ‚úì Best parameters: {'C': np.float64(1.5409457762881555), 'max_iter': 3000, 'solver': 'lbfgs'}
   ‚Ä¢ Iterations to convergence: [array([191], dtype=int32)]
   ‚úì Training completed successfully

[2/4] Training Random Forest...
   üîç Performing hyperparameter tuning (n_iter=50, cv=5)...
   ‚úì Best CV score: 0.8545
   ‚úì Best parameters: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 7, 'n_estimators': 277}
   ‚úì Training completed successfully

[3/4] Training Gradient Boosting...
   üîç Performing hyperparameter tuning (n_iter=50, cv=5)...
   ‚úì Best CV score: 0.8540
   ‚úì Best parameters: {'learning_rate': np.float64(0.062009396052331626), 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 3, 'n_estimators': 153}
   ‚úì Training completed successfully

[4/4] Training XGBoost...
   üîç Performing hyperparameter tuning (n_iter=50, cv=5)...
   ‚úì Best CV score: 0.8555
   ‚úì Best parameters: {'colsample_bytree': np.float64(0.8285733635843882), 'learning_rate': np.float64(0.14763327094232792), 'max_depth': 3, 'n_estimators': 101, 'subsample': np.float64(0.944399754453365)}
   ‚úì Training completed successfully

======================================================================
‚úì Training completed: 4/4 models trained successfully
======================================================================
   ‚úì Models trained: 4

[Step 5/7] Evaluating models...
======================================================================
STEP 6: MODEL EVALUATION
======================================================================

[1/4] Evaluating Logistic Regression...
   üìä Core Metrics:
      ‚Ä¢ Accuracy:  0.8305 (83.05%)
      ‚Ä¢ Precision: 0.8466 (84.66%)
      ‚Ä¢ Recall:    0.8452 (84.52%)
      ‚Ä¢ F1-Score:  0.8459 (84.59%)
   üîí Security Metrics:
      ‚Ä¢ Specificity: 0.8125 (81.25%)
      ‚Ä¢ FPR:         0.1875 (18.75%)
      ‚Ä¢ FNR:         0.1548 (15.48%)
   ‚öñÔ∏è  Balanced Metrics:
      ‚Ä¢ Balanced Acc: 0.8288 (82.88%)
      ‚Ä¢ MCC:          0.6575
   üìà AUC Metrics:
      ‚Ä¢ ROC-AUC:      0.8910
      ‚Ä¢ PR-AUC:       0.8706

[2/4] Evaluating Random Forest...
   üìä Core Metrics:
      ‚Ä¢ Accuracy:  0.8386 (83.86%)
      ‚Ä¢ Precision: 0.8731 (87.31%)
      ‚Ä¢ Recall:    0.8270 (82.70%)
      ‚Ä¢ F1-Score:  0.8494 (84.94%)
   üîí Security Metrics:
      ‚Ä¢ Specificity: 0.8528 (85.28%)
      ‚Ä¢ FPR:         0.1472 (14.72%)
      ‚Ä¢ FNR:         0.1730 (17.30%)
   ‚öñÔ∏è  Balanced Metrics:
      ‚Ä¢ Balanced Acc: 0.8399 (83.99%)
      ‚Ä¢ MCC:          0.6769
   üìà AUC Metrics:
      ‚Ä¢ ROC-AUC:      0.8910
      ‚Ä¢ PR-AUC:       0.8704

[3/4] Evaluating Gradient Boosting...
   üìä Core Metrics:
      ‚Ä¢ Accuracy:  0.8381 (83.81%)
      ‚Ä¢ Precision: 0.8696 (86.96%)
      ‚Ä¢ Recall:    0.8304 (83.04%)
      ‚Ä¢ F1-Score:  0.8496 (84.96%)
   üîí Security Metrics:
      ‚Ä¢ Specificity: 0.8475 (84.75%)
      ‚Ä¢ FPR:         0.1525 (15.25%)
      ‚Ä¢ FNR:         0.1696 (16.96%)
   ‚öñÔ∏è  Balanced Metrics:
      ‚Ä¢ Balanced Acc: 0.8390 (83.90%)
      ‚Ä¢ MCC:          0.6753
   üìà AUC Metrics:
      ‚Ä¢ ROC-AUC:      0.8898
      ‚Ä¢ PR-AUC:       0.8644

[4/4] Evaluating XGBoost...
   üìä Core Metrics:
      ‚Ä¢ Accuracy:  0.8390 (83.90%)
      ‚Ä¢ Precision: 0.8711 (87.11%)
      ‚Ä¢ Recall:    0.8304 (83.04%)
      ‚Ä¢ F1-Score:  0.8503 (85.03%)
   üîí Security Metrics:
      ‚Ä¢ Specificity: 0.8496 (84.96%)
      ‚Ä¢ FPR:         0.1504 (15.04%)
      ‚Ä¢ FNR:         0.1696 (16.96%)
   ‚öñÔ∏è  Balanced Metrics:
      ‚Ä¢ Balanced Acc: 0.8400 (84.00%)
      ‚Ä¢ MCC:          0.6774
   üìà AUC Metrics:
      ‚Ä¢ ROC-AUC:      0.8857
      ‚Ä¢ PR-AUC:       0.8630

======================================================================

[Step 6/7] Comparing models and selecting best...
======================================================================
MODEL COMPARISON TABLE - COMPREHENSIVE METRICS
======================================================================

üìä Core Metrics (Accuracy, Precision, Recall, F1-Score):
                     accuracy  precision  recall      f1
XGBoost                0.8390     0.8711  0.8304  0.8503
Random Forest          0.8386     0.8731  0.8270  0.8494
Gradient Boosting      0.8381     0.8696  0.8304  0.8496
Logistic Regression    0.8305     0.8466  0.8452  0.8459

üîí Security Metrics (Specificity, FPR, FNR):
                     specificity     fpr     fnr
XGBoost                   0.8496  0.1504  0.1696
Random Forest             0.8528  0.1472  0.1730
Gradient Boosting         0.8475  0.1525  0.1696
Logistic Regression       0.8125  0.1875  0.1548

‚öñÔ∏è  Balanced Metrics (Balanced Accuracy, MCC):
                     balanced_accuracy     mcc
XGBoost                         0.8400  0.6774
Random Forest                   0.8399  0.6769
Gradient Boosting               0.8390  0.6753
Logistic Regression             0.8288  0.6575

üìà AUC Metrics (ROC-AUC, PR-AUC):
                     roc_auc  pr_auc
XGBoost               0.8857  0.8630
Random Forest         0.8910  0.8704
Gradient Boosting     0.8898  0.8644
Logistic Regression   0.8910  0.8706
======================================================================
======================================================================
STEP 7: BEST MODEL SELECTION
======================================================================

üìä Selection Method: Comprehensive evaluation across all metrics
   Primary criterion: Accuracy (highest: XGBoost)
   Secondary: Balanced performance across precision, recall, and security metrics

üèÜ Best Model: XGBoost
   Model Type: XGBClassifier
   Number of Estimators: 101

üìä Core Performance Metrics:
   ‚Ä¢ Accuracy:  0.8390 (83.90%)
   ‚Ä¢ Precision: 0.8711 (87.11%)
   ‚Ä¢ Recall:    0.8304 (83.04%)
   ‚Ä¢ F1-Score:  0.8503 (85.03%)

üîí Security-Focused Metrics:
   ‚Ä¢ Specificity: 0.8496 (84.96%)
   ‚Ä¢ FPR:         0.1504 (15.04%)
   ‚Ä¢ FNR:         0.1696 (16.96%)

‚öñÔ∏è  Balanced Metrics:
   ‚Ä¢ Balanced Acc: 0.8400 (84.00%)
   ‚Ä¢ MCC:          0.6774

üìà AUC Metrics:
   ‚Ä¢ ROC-AUC:      0.8857 (88.57%)
   ‚Ä¢ PR-AUC:       0.8630 (86.30%)

======================================================================

[Step 7/7] Generating final validation and reporting...

======================================================================
DETAILED CLASSIFICATION REPORT: XGBoost
======================================================================

              precision    recall  f1-score   support

    Phishing       0.80      0.85      0.83       944
  Legitimate       0.87      0.83      0.85      1156

    accuracy                           0.84      2100
   macro avg       0.84      0.84      0.84      2100
weighted avg       0.84      0.84      0.84      2100

======================================================================

======================================================================
PIPELINE EXECUTION COMPLETED
======================================================================

‚úÖ Machine Learning Pipeline completed successfully!
   ‚Ä¢ Data loaded and preprocessed
   ‚Ä¢ 4 models trained
   ‚Ä¢ All models evaluated with comprehensive metrics
   ‚Ä¢ Best model selected: XGBoost
   ‚Ä¢ Best accuracy: 83.90%
======================================================================

‚úÖ Pipeline execution completed successfully.
